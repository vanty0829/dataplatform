{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class meta:\n",
    "    def __init__(self, data):\n",
    "        # Check if data is a dictionary\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                setattr(self, key, value)\n",
    "        else:\n",
    "            raise ValueError(\"Expected a dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = meta({\n",
    "    \"source\":[\n",
    "        \"bronze.dim1\",\n",
    "        \"bronze.dim2\"\n",
    "    ],\n",
    "    \"target\":[\n",
    "        \"silver.dim2\",\n",
    "        \"silver.dim3\",\n",
    "        \"silver.dim4\"\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "     ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/11.6 MB 1.3 MB/s eta 0:00:10\n",
      "     ---------------------------------------- 0.1/11.6 MB 1.4 MB/s eta 0:00:08\n",
      "      --------------------------------------- 0.2/11.6 MB 2.1 MB/s eta 0:00:06\n",
      "     - -------------------------------------- 0.5/11.6 MB 2.9 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.8/11.6 MB 3.4 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.3/11.6 MB 4.6 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.2/11.6 MB 7.0 MB/s eta 0:00:02\n",
      "     --------- ------------------------------ 2.7/11.6 MB 7.5 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 3.6/11.6 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.1/11.6 MB 8.7 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 5.7/11.6 MB 11.1 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 6.5/11.6 MB 11.9 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 7.9/11.6 MB 13.3 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 9.1/11.6 MB 14.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 10.5/11.6 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  11.6/11.6 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 11.6/11.6 MB 21.1 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "     ---------------------------------------- 0.0/346.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 346.6/346.6 kB 21.0 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "     ---------------------------------------- 0.0/508.0 kB ? eta -:--:--\n",
      "     ------------------------------------- 508.0/508.0 kB 31.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vanty\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting numpy>=1.22.4\n",
      "  Downloading numpy-2.1.2-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.2/12.9 MB 25.5 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 2.3/12.9 MB 24.6 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 3.2/12.9 MB 25.1 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.6/12.9 MB 24.7 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.0/12.9 MB 27.4 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.9/12.9 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 8.1/12.9 MB 24.6 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.8/12.9 MB 23.5 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.9 MB 23.4 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.5/12.9 MB 22.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 11.4/12.9 MB 22.6 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.2/12.9 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.9/12.9 MB 21.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.9/12.9 MB 19.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vanty\\.pyenv\\pyenv-win\\versions\\3.10.11\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.1.2 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'c:\\Users\\vanty\\.pyenv\\pyenv-win\\versions\\3.10.11\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
      "[notice] To update, run: c:\\Users\\vanty\\.pyenv\\pyenv-win\\versions\\3.10.11\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "def read_script_from_s3(bucket_name, object_key):\n",
    "    s3_client = boto3.client('s3')\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    return response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Specify your S3 bucket and object key (file path)\n",
    "bucket_name = 'your-bucket-name'\n",
    "object_key = 'path/to/your/delta.gold.testing.py'\n",
    "\n",
    "# Read the PySpark script from S3\n",
    "pyspark_script = read_script_from_s3(bucket_name, object_key)\n",
    "\n",
    "# Extract SQL queries\n",
    "sql_query_pattern = r\"(?<=spark.sql\\(f\\\"\\\"\\\"\\n)(.*?)(?=\\\"\\\"\\\"\\))\"\n",
    "sql_queries_matches = re.findall(sql_query_pattern, pyspark_script, re.DOTALL)\n",
    "\n",
    "all_source_tables = set()\n",
    "\n",
    "# Process each SQL query\n",
    "for sql_query in sql_queries_matches:\n",
    "    # Regex pattern to find tables in FROM and JOIN clauses\n",
    "    combined_pattern = r\"from\\s+([a-zA-Z0-9_.]+)|(?:left join|right join|inner join|full outer join|cross join|left anti join|right anti join)\\s+([a-zA-Z0-9_.]+)\"\n",
    "    combined_matches = re.findall(combined_pattern, sql_query, re.IGNORECASE)\n",
    "\n",
    "    # Add the found tables to the set for uniqueness\n",
    "    for match in combined_matches:\n",
    "        if match[0]:  # Main source table\n",
    "            all_source_tables.add(match[0])\n",
    "        if match[1]:  # Join tables\n",
    "            all_source_tables.add(match[1])\n",
    "\n",
    "# Get the target table name from the object key (file name)\n",
    "target_table_name = object_key.split('/')[-1].split('.')[0]  # Remove file extension\n",
    "\n",
    "# Prepare DataFrame\n",
    "source_table_list = list(all_source_tables)\n",
    "df = pd.DataFrame({'source_table': source_table_list, 'target_table': target_table_name})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def read_script_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Path to your PySpark script file\n",
    "script_file_path = './delta.gold.testing.py'\n",
    "\n",
    "# Read the PySpark script\n",
    "pyspark_script = read_script_from_file(script_file_path)\n",
    "\n",
    "\n",
    "\n",
    "sql_query_pattern = r\"(?<=spark.sql\\(f\\\"\\\"\\\"\\n)(.*?)(?=\\\"\\\"\\\"\\))\"\n",
    "sql_queries_matches = re.findall(sql_query_pattern, pyspark_script, re.DOTALL)\n",
    "\n",
    "all_source_tables = set()\n",
    "\n",
    "# Process each SQL query\n",
    "for idx, sql_query in enumerate(sql_queries_matches):\n",
    "\n",
    "\n",
    "    # Regex pattern to find tables in FROM and JOIN clauses\n",
    "    combined_pattern = r\"from\\s+([a-zA-Z0-9_.]+)|(?:left join|right join|inner join|full outer join|cross join|left anti join|right anti join)\\s+([a-zA-Z0-9_.]+)\"\n",
    "    combined_matches = re.findall(combined_pattern, sql_query, re.IGNORECASE)\n",
    "\n",
    "    # Add the found tables to the set for uniqueness\n",
    "    for match in combined_matches:\n",
    "        if match[0]:  # Main source table\n",
    "            all_source_tables.add(match[0])\n",
    "        if match[1]:  # Join tables\n",
    "            all_source_tables.add(match[1])\n",
    "\n",
    "target_table_name = script_file_path.split('/')[-1].replace('.py','')  # Remove file extension\n",
    "\n",
    "# Prepare DataFrame\n",
    "source_table_list = list(all_source_tables)\n",
    "df = pd.DataFrame({'source_table': source_table_list, 'target_table': target_table_name})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_table</th>\n",
       "      <th>target_table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>delta.GOLD.D_TRANSACTION_TYPE</td>\n",
       "      <td>delta.gold.testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>delta.GOLD.D_CUSTOMER</td>\n",
       "      <td>delta.gold.testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>delta.SILVER.FHO1_FUNDS_TRANSFER000</td>\n",
       "      <td>delta.gold.testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>delta.GOLD.D_PMH_TRANSACTION_TYPE</td>\n",
       "      <td>delta.gold.testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>delta.bronze.FBNK_TRANSACTION</td>\n",
       "      <td>delta.gold.testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>delta.GOLD.D_ACCOUNT</td>\n",
       "      <td>delta.gold.testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>delta.SILVER.PMH_TRANSACTION_TYPE</td>\n",
       "      <td>delta.gold.testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>delta.SILVER.FHO1_STMT_ENTRY</td>\n",
       "      <td>delta.gold.testing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>delta.GOLD.D_COMPANY</td>\n",
       "      <td>delta.gold.testing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          source_table        target_table\n",
       "0        delta.GOLD.D_TRANSACTION_TYPE  delta.gold.testing\n",
       "1                delta.GOLD.D_CUSTOMER  delta.gold.testing\n",
       "2  delta.SILVER.FHO1_FUNDS_TRANSFER000  delta.gold.testing\n",
       "3    delta.GOLD.D_PMH_TRANSACTION_TYPE  delta.gold.testing\n",
       "4        delta.bronze.FBNK_TRANSACTION  delta.gold.testing\n",
       "5                 delta.GOLD.D_ACCOUNT  delta.gold.testing\n",
       "6    delta.SILVER.PMH_TRANSACTION_TYPE  delta.gold.testing\n",
       "7         delta.SILVER.FHO1_STMT_ENTRY  delta.gold.testing\n",
       "8                 delta.GOLD.D_COMPANY  delta.gold.testing"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'delta.GOLD.D_ACCOUNT',\n",
       " 'delta.GOLD.D_COMPANY',\n",
       " 'delta.GOLD.D_CUSTOMER',\n",
       " 'delta.GOLD.D_PMH_TRANSACTION_TYPE',\n",
       " 'delta.GOLD.D_TRANSACTION_TYPE',\n",
       " 'delta.SILVER.FHO1_FUNDS_TRANSFER000',\n",
       " 'delta.SILVER.FHO1_STMT_ENTRY',\n",
       " 'delta.SILVER.PMH_TRANSACTION_TYPE',\n",
       " 'delta.bronze.FBNK_TRANSACTION'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_source_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the dependency string\n",
    "dependency_string = \"a,b|b,c|a,d\"\n",
    "\n",
    "# Step 1: Parse the dependency string\n",
    "dependencies = dependency_string.split('|')\n",
    "task_dependencies = [dep.split(',') for dep in dependencies]\n",
    "\n",
    "# Step 2: Create the DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2023, 10, 1),\n",
    "}\n",
    "\n",
    "with DAG('dynamic_dependency_dag', default_args=default_args, schedule_interval='@daily') as dag:\n",
    "    # Create a dictionary to hold task references\n",
    "    tasks = {}\n",
    "\n",
    "    # Create tasks based on unique identifiers\n",
    "    for task_pair in task_dependencies:\n",
    "        for task_id in task_pair:\n",
    "            if task_id not in tasks:\n",
    "                tasks[task_id] = DummyOperator(task_id=task_id)\n",
    "\n",
    "    # Set task dependencies\n",
    "    for task_pair in task_dependencies:\n",
    "        if len(task_pair) == 2:\n",
    "            tasks[task_pair[0]] >> tasks[task_pair[1]]\n",
    "\n",
    "# This DAG will now reflect the dependencies:\n",
    "# a >> b\n",
    "# b >> c\n",
    "# a >> d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from airflow import DAG\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Create a DataFrame from the dependency string\n",
    "dependency_string = \"a,b|b,c|a,d\"\n",
    "dependencies = [dep.split(',') for dep in dependency_string.split('|')]\n",
    "\n",
    "# Create a DataFrame\n",
    "df_dependencies = pd.DataFrame(dependencies, columns=['task_from', 'task_to'])\n",
    "\n",
    "# Step 2: Generate the Airflow DAG\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2023, 10, 1),\n",
    "}\n",
    "\n",
    "with DAG('dynamic_dependency_dag', default_args=default_args, schedule_interval='@daily') as dag:\n",
    "    tasks = {}\n",
    "\n",
    "    # Create tasks based on unique identifiers\n",
    "    for _, row in df_dependencies.iterrows():\n",
    "        from_task = row['task_from']\n",
    "        to_task = row['task_to']\n",
    "\n",
    "        if from_task not in tasks:\n",
    "            tasks[from_task] = DummyOperator(task_id=from_task)\n",
    "\n",
    "        if to_task not in tasks:\n",
    "            tasks[to_task] = DummyOperator(task_id=to_task)\n",
    "\n",
    "        # Set the dependency\n",
    "        tasks[from_task] >> tasks[to_task]\n",
    "\n",
    "# The DAG will have the following dependencies:\n",
    "# a >> b\n",
    "# b >> c\n",
    "# a >> d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['silver.dim2', 'silver.dim3', 'silver.dim4']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bronze.dim1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for src in meta.source:\n",
    "    for tgt in meta.target:\n",
    "        result.append([src, tgt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bronze.dim1', 'silver.dim2'],\n",
       " ['bronze.dim1', 'silver.dim3'],\n",
       " ['bronze.dim1', 'silver.dim4'],\n",
       " ['bronze.dim2', 'silver.dim2'],\n",
       " ['bronze.dim2', 'silver.dim3'],\n",
       " ['bronze.dim2', 'silver.dim4']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
